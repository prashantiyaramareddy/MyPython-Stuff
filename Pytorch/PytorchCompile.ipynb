{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkEYGmXZ2/bknIuOQD9NHg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prashantiyaramareddy/MyPython-Stuff/blob/master/Pytorch/PytorchCompile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch torch.Compile"
      ],
      "metadata": {
        "id": "vbCeCYYbA7go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's happening behind the scenes of torch.compile()?\n",
        "\n",
        "torch.compile(0 is designed to \"just work\" but there are few technologies behind it:\n",
        "* TorchDynamo\n",
        "* AOTAutograd\n",
        "* PrimTorch\n",
        "* TorchInductor\n",
        "\n",
        "From a high level, the two main improvements torch.compile() offers are:\n",
        "* Fusion ( or operator fusion)\n",
        "* Graph capture (or graph tracing)\n",
        "\n"
      ],
      "metadata": {
        "id": "ui0lfTZWB7-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we're using a NVIDIA GPU\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find(\"failed\") >= 0:\n",
        "    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
        "\n",
        "  # Get GPU name\n",
        "  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "  gpu_name = gpu_name[1]\n",
        "  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n",
        "  print(f'GPU name: {GPU_NAME}')\n",
        "\n",
        "  # Get GPU capability score\n",
        "  GPU_SCORE = torch.cuda.get_device_capability()\n",
        "  print(f\"GPU capability score: {GPU_SCORE}\")\n",
        "  if GPU_SCORE >= (8, 0):\n",
        "    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n",
        "  else:\n",
        "    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n",
        "\n",
        "  # Print GPU info\n",
        "  print(f\"GPU information:\\n{gpu_info}\")\n",
        "\n",
        "else:\n",
        "  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks-TzhCkJUfs",
        "outputId": "caddf6d2-85ea-45f5-ed90-c1023ccc0c9e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU name: Tesla_T4\n",
            "GPU capability score: (7, 5)\n",
            "GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\n",
            "GPU information:\n",
            "Wed Oct 29 08:55:10 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8             13W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKCqVyLbLA3D",
        "outputId": "db02c770-4f5a-4fb9-ab31-5e7810b8eb21"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Globally set devices"
      ],
      "metadata": {
        "id": "RbP_f_jILDA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of my favourite new features in PyTorch 2.x is being able to set the default device type via:\n",
        "\n",
        "* Context manager\n",
        "* Globally\n",
        "Previously, you could only set the default device type via:\n",
        "\n",
        "tensor.to(device)"
      ],
      "metadata": {
        "id": "KT1p_qoVMZxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Set the device with context manager (requires PyTorch 2.x+)\n",
        "with torch.device(device):\n",
        "    # All tensors created in this block will be on device\n",
        "    layer = torch.nn.Linear(20, 30)\n",
        "    print(f\"Layer weights are on device: {layer.weight.device}\")\n",
        "    print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZx17oiBMl4-",
        "outputId": "09475366-db10-4538-9e45-9882a53c399c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer weights are on device: cuda:0\n",
            "Layer creating data on device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now back to CPU"
      ],
      "metadata": {
        "id": "UNsN2Si1M3D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set the device globally\n",
        "torch.set_default_device(\"cpu\")\n",
        "\n",
        "# All tensors created will be on \"cpu\"\n",
        "layer = torch.nn.Linear(20, 30)\n",
        "print(f\"Layer weights are on device: {layer.weight.device}\")\n",
        "print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn858rkyMuEN",
        "outputId": "ab6441a5-b05e-4582-d20d-70ac099543f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer weights are on device: cpu\n",
            "Layer creating data on device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Setting up the experiments\n",
        "\n",
        "To keep things simple, as we discussed we are going to run a series of four experiments\n",
        "\n",
        "* Model : ResNet50 (from TorchVision)\n",
        "* Data : CIFAR10 (from TorchVision)\n",
        "* Epochs: 5(single run) and 3X5 (multiple runs)\n",
        "Batch size : 128\n",
        "Image Size:224\n",
        "\n",
        "Each experiment will run with and without torch.compile().torch\n",
        "\n",
        "Why the single and multiple runs?\n",
        "\n",
        "Because we can measure speedups via a single run, however, we'll also want to run\n",
        " the tests multiple times to get an average (just to make\n",
        "sure the results from a single run weren't a fluke or something went wrong)."
      ],
      "metadata": {
        "id": "CUteJLoGM9Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"TorchVision version: {torchvision.__version__}\")\n",
        "\n",
        "# Set the target device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar0u--x1Nzzc",
        "outputId": "88118701-24da-4dff-a0b0-23155cfba932"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "TorchVision version: 0.23.0+cu126\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Create model and transforms\n",
        "\n"
      ],
      "metadata": {
        "id": "OoJsq5i1N8gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model weights and transforms\n",
        "model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
        "transforms = model_weights.transforms()\n",
        "\n",
        "# Setup model\n",
        "model = torchvision.models.resnet50(weights=model_weights)\n",
        "model.to(device)\n",
        "\n",
        "# Count the number of parameters in the model\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in the model: {total_params}\")\n",
        "print(f\"Model Transforms: {transforms}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_7fZAfcOF2i",
        "outputId": "efa9cc1f-21cc-4e77-8084-ed27c05681e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 192MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in the model: 25557032\n",
            "Model Transforms: ImageClassification(\n",
            "    crop_size=[224]\n",
            "    resize_size=[232]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BILINEAR\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's turn the above code into function so we can replicate it later"
      ],
      "metadata": {
        "id": "m1-EE8qxPtNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_classes = 10):\n",
        "  \"\"\"\n",
        "  Creates a ResNet50 model with the latest weights and transforms via torchvision.\n",
        "  \"\"\"\n",
        "\n",
        "  model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
        "  transforms = model_weights.transforms()\n",
        "  model = torchvision.models.resnet50(weights=model_weights)\n",
        "  model.to(device)\n",
        "\n",
        "  # Adjust the number of output features in model to match the number of classes in the dataset\n",
        "  model.fc = torch.nn.Linear(in_features=2048, out_features=num_classes)\n",
        "\n",
        "  return model, transforms\n",
        "\n",
        "\n",
        "model, transforms = create_model()"
      ],
      "metadata": {
        "id": "TtabuRRXPiz2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 Speedups are most noticeable when a large portion of the GPU is being used\n",
        "Since modern GPUs are so fast at performing operations, you will often notice the majority of relative speedups when as much data as possible is on the GPU.\n",
        "\n",
        "This can be achieved by:\n",
        "\n",
        " * **Increasing the batch size**- More samples per batch means more samples on the GPU, for example, using a batch size of 256 instead of 32.\n",
        "* **Increasing data size** - For example, using larger image size, 224x224 instead of 32x32. A larger data size means that more tensor operations will be happening on the GPU.\n",
        "\n",
        "* **Increasing model size** - For example, using a larger model such as ResNet101 instead of ResNet50. A larger model means that more tensor operations will be happening on the GPU.\n",
        "* **Decreasing data transfer** - For example, setting up all your tensors to be on GPU memory, this minimizes the amount of data transfer between the CPU and GPU."
      ],
      "metadata": {
        "id": "oL2Y_fwWQ8A-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Checking the memory limits of our GPU"
      ],
      "metadata": {
        "id": "sO9Ry82YTAsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the available GPU memory and total GPU memory\n",
        "\n",
        "total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n",
        "print(f\"Total free GPU memory: {total_free_gpu_memory / 1024**2:.2f} MB\")\n",
        "print(f\"Total GPU memory: {total_gpu_memory / 1024**2:.2f} MB\")\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8a0yGHkT2Ct",
        "outputId": "e2b60b52-2533-4392-ebf1-257772385308"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total free GPU memory: 14764.12 MB\n",
            "Total GPU memory: 15095.06 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The takeaways here are:\n",
        "\n",
        "* The higher the memory available on your GPU, the bigger your batch size can be, the bigger your model can be, **the bigger your data samples can be.**\n",
        "* For speedups, you should always be trying to use **as much of the GPU(s) as possible.**"
      ],
      "metadata": {
        "id": "JPh_5hOzUcWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size depending on amount of GPU memory\n",
        "total_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3)\n",
        "if total_free_gpu_memory_gb >= 16:\n",
        "  BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.\n",
        "  IMAGE_SIZE = 224\n",
        "  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")\n",
        "else:\n",
        "  BATCH_SIZE = 32\n",
        "  IMAGE_SIZE = 128\n",
        "  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I13I3NAeUVNy",
        "outputId": "06095eac-67b2-42ff-98ec-322da8b22165"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory available is 15.481 GB, using batch size of 32 and image size 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 More potential speedups with TF32\n",
        "TF32 stands for TensorFloat-32, a data format which is a combination of 16-bit and 32-bit floating point numbers.\n",
        "\n",
        "You can read more about how it works on NVIDIA's blog.\n",
        "\n",
        "The main thing you should know is that it allows you to perform faster matrix multiplications on GPUs with the Ampere architecture and above (a compute capability score of 8.0+).\n",
        "\n",
        "Although it's not specific to PyTorch 2.0, since we're talking about newer GPUs, it's worth mentioning.\n",
        "\n",
        "If you're using a GPU with a compute capability score of 8.0 or above, you can enable TF32 by setting torch.backends.cuda.matmul.allow_tf32 = True (this defaults to False).\n",
        "\n",
        "Let's write a check that sets it automatically for us based on our GPUs compute capability score."
      ],
      "metadata": {
        "id": "p0vno0cnVVDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if GPU_SCORE >= (8, 0):\n",
        "  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, enabling TensorFloat32 (TF32) computing (faster on new GPUs)\")\n",
        "  torch.backends.cuda.matmul.allow_tf32 = True\n",
        "else:\n",
        "  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, TensorFloat32 (TF32) not available, to use it you need a GPU with score >= (8, 0)\")\n",
        "  torch.backends.cuda.matmul.allow_tf32 = False\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqwChiRpVePg",
        "outputId": "c00b229a-369f-4582-af48-fe46ee865194"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU with score: (7, 5), TensorFloat32 (TF32) not available, to use it you need a GPU with score >= (8, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XsuzCtoAV54q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Preparing datasets\n",
        "Computing setup done!\n",
        "\n",
        "Let's now create our datasets.\n",
        "\n",
        "To keep things simple, we'll use CIFAR10 since it's readily available in torchvision.\n",
        "\n",
        "Some info about CIFAR10 the CIFAR10 website:\n",
        "\n",
        "CIFAR10 is a dataset of 60,000 32x32 color images in 10 classes, with 6,000 images per class.\n",
        "There are 50,000 training images and 10,000 test images.\n",
        "The dataset contains 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.\n",
        "Although the original dataset consists of 32x32 images, we'll use the transforms we created earlier to resize them to 224x224 (larger images provide more information and will take up more memory on the GPU)."
      ],
      "metadata": {
        "id": "GLGXOMVRVnY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test datasets\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='.',\n",
        "                                             train=True,\n",
        "                                             download=True,\n",
        "                                             transform=transforms)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='.',\n",
        "                                            train=False, # want the test split\n",
        "                                            download=True,\n",
        "                                            transform=transforms)\n",
        "\n",
        "# Get the lengths of the datasets\n",
        "train_len = len(train_dataset)\n",
        "test_len = len(test_dataset)\n",
        "\n",
        "print(f\"[INFO] Train dataset length: {train_len}\")\n",
        "print(f\"[INFO] Test dataset length: {test_len}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzFZdzgCVlPq",
        "outputId": "fd59cac3-08d1-4f53-9844-ae14eae4cac4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:06<00:00, 26.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Train dataset length: 50000\n",
            "[INFO] Test dataset length: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Create DataLoaders"
      ],
      "metadata": {
        "id": "1FofHCtLWAhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally GPUs aren't the bottleneck of machine learning code.\n",
        "\n",
        "Data loading is the main bottleneck.\n",
        "\n",
        "As in, the transfer speed from CPU to GPU.\n",
        "\n",
        "As we've discussed before you want to get your data to the GPU as fast as possible.\n",
        "\n",
        "Let's create our DataLoaders using torch.utils.data.DataLoader.\n",
        "\n",
        "We'll set their batch_size to the BATCH_SIZE we created earlier.\n",
        "\n",
        "And the num_workers parameter to be the number of CPU cores we have available with os.cpu_count()."
      ],
      "metadata": {
        "id": "TC_XeAMLWHDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create DataLoaders\n",
        "import os\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "train_dataloader = DataLoader(dataset= train_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True,\n",
        "                              num_workers=NUM_WORKERS)\n",
        "\n",
        "test_dataloader  = DataLoader(dataset=test_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True,\n",
        "                              num_workers=NUM_WORKERS)\n",
        "\n",
        "print(f\"Train dataloader length: {len(train_dataloader)} batches of {BATCH_SIZE} samples\")\n",
        "print(f\"Test dataloader length: {len(test_dataloader)} batches of {BATCH_SIZE} samples\")\n",
        "print(f\"Using number of workers: {NUM_WORKERS}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK8qFVMWWWU9",
        "outputId": "367148b7-9149-4a11-b295-07c796ea9572"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataloader length: 1563 batches of 32 samples\n",
            "Test dataloader length: 313 batches of 32 samples\n",
            "Using number of workers: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 2.7 Creating training and testing loops\n",
        "\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def train_step(epoch: int,\n",
        "               model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn : torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device,\n",
        "               disable_progress_bar:bool = False) -> Tuple[float, float]:\n",
        "  \"\"\"\n",
        "  Trains a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to training mode and then\n",
        "  runs through all of the required training steps (forward\n",
        "  pass, loss calculation, optimizer step).\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "  \"\"\"\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0,0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  progress_bar = tqdm(\n",
        "      enumarate(dataloader),\n",
        "      desc=f\"Epoch: {epoch} | Train\",\n",
        "      total=len(dataloader),\n",
        "      disable=disable_progress_bar\n",
        "  )\n",
        "\n",
        "  for batch, (X,y) in progress_bar:\n",
        "    # Send data to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # 2. Calculate and accumulate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer."
      ],
      "metadata": {
        "id": "0P47HYMjXCQG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}